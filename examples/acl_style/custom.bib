@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Processing of the 31st Conference on Neural Information Processing Systems (NeurIPS)},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  pages={1--9},
  year={2019}
}

@inproceedings{bao2020unilmv2,
  title={Unilmv2: Pseudo-masked language models for unified language model pre-training},
  author={Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Piao, Songhao and Zhou, Ming and others},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages={642--652},
  year={2020}
}

@inproceedings{he2021deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle={Proceedings of the 2021 International Conference on Learning Representations (ICLR)},
  pages={1--21},
  year={2021}
}

@article{sun2019ernie,
  title={ERNIE: Enhanced representation through knowledge integration},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@inproceedings{jain2019multimodal,
  title={Multimodal document image classification},
  author={Jain, Rajiv and Wigington, Curtis},
  booktitle={Proceedings of the 2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={71--77},
  year={2019}
}

@inproceedings{katti2018chargrid,
  title={Chargrid: Towards Understanding 2D Documents},
  author={Katti, Anoop R and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4459--4469},
  year={2018}
}

@inproceedings{yang2017learning,
  title={Learning to extract semantic structure from documents using multimodal fully convolutional neural networks},
  author={Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Lee Giles, C},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={5315--5324},
  year={2017}
}


@inproceedings{sarkhel2019deterministic,
  title={Deterministic routing between layout abstractions for multi-scale classification of visually rich documents},
  author={Sarkhel, Ritesh and Nandi, Arnab},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2019}
}

@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics (NAACL)},
  pages={1480--1489},
  year={2016}
}

@inproceedings{cheng2020one,
  title={One-shot Text Field Labeling using Attention and Belief Propagation for Structure Information Extraction},
  author={Cheng, Mengli and Qiu, Minghui and Shi, Xing and Huang, Jun and Lin, Wei},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia (MM)},
  pages={340--348},
  year={2020}
}

@article{xu2021layoutxlm,
  title={LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding},
  author={Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08836},
  year={2021}
}

@article{hong2021bros,
  title={BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents},
  author={Hong, Teakgyu and Kim, Donghyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
  journal={arXiv preprint arXiv:2108.04539},
  year={2021}
}

@inproceedings{powalski2021going,
  title={Going full-tilt boogie on document understanding with text-image-layout transformer},
  author={Powalski, Rafa{\l} and Borchmann, {\L}ukasz and Jurkiewicz, Dawid and Dwojak, Tomasz and Pietruszka, Micha{\l} and Pa{\l}ka, Gabriela},
  booktitle={Proceedings of the 2021 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={732--747},
  year={2021}
}

@inproceedings{park2019cord,
  title={CORD: A Consolidated Receipt Dataset for Post-OCR Parsing},
  author={Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  booktitle={Workshop on Document Intelligence at NeurIPS 2019},
  year={2019}
}

@inproceedings{huang2019icdar2019,
  title={Icdar2019 competition on scanned receipt ocr and information extraction},
  author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  booktitle={Proceedings of the 2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1516--1520},
  year={2019}
}

@inproceedings{gralinski2021kleister,
  title={Kleister: A novel task for information extraction involving long documents with complex layout},
  author={Grali{\'n}ski, Filip and Stanis{\l}awek, Tomasz and Wr{\'o}blewska, Anna and Lipi{\'n}ski, Dawid and Kaliska, Agnieszka and Rosalska, Paulina and Topolski, Bartosz and Biecek, Przemys{\l}aw},
  booktitle={Proceedings of the 2021 International Conference on Document Analysis and Recognition (ICDAR)},
  year={2021}
}

@inproceedings{harley2015evaluation,
  title={Evaluation of deep convolutional nets for document image classification and retrieval},
  author={Harley, Adam W and Ufkes, Alex and Derpanis, Konstantinos G},
  booktitle={Proceedings of the 13th International Conference on Document Analysis and Recognition (ICDAR)},
  pages={991--995},
  year={2015}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages={2200--2209},
  year={2021}
}

@inproceedings{biten2019icdar,
  title={Icdar 2019 competition on scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Mathew, Minesh and Jawahar, CV and Valveny, Ernest and Karatzas, Dimosthenis},
  booktitle={Proceedings of the 2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1563--1570},
  year={2019}
}

@inproceedings{lewis2006building,
  title={Building a test collection for complex document information processing},
  author={Lewis, David and Agam, Gady and Argamon, Shlomo and Frieder, Ophir and Grossman, David and Heard, Jefferson},
  booktitle={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR)},
  pages={665--666},
  year={2006}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Processing of the 29th Conference on Neural Information Processing Systems (NeurIPS)},
  pages={1--9},
  year={2015}
}


@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{xu2020layoutlm,
  title={Layoutlm: Pre-training of text and layout for document image understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  pages={1192--1200},
  year={2020}
}

@inproceedings{xu2021layoutlmv2,
  title={LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={2579--2591},
  year={2021}
}

@inproceedings{li2021structurallm,
  title={StructuralLM: Structural Pre-training for Form Understanding},
  author={Li, Chenliang and Bi, Bin and Yan, Ming and Wang, Wei and Huang, Songfang and Huang, Fei and Si, Luo},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={6309--6318},
  year={2021}
}

@inproceedings{li2021structext,
  title={StrucTexT: Structured Text Understanding with Multi-Modal Transformers},
  author={Li, Yulin and Qian, Yuxi and Yu, Yuechen and Qin, Xiameng and Zhang, Chengquan and Liu, Yan and Yao, Kun and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia (MM)},
  pages={1912--1920},
  year={2021}
}


@inproceedings{li2021selfdoc,
  title={SelfDoc: Self-Supervised Document Representation Learning},
  author={Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={5652--5660},
  year={2021}
}

@inproceedings{appalaraju2021docformer,
  title={Docformer: End-to-end transformer for document understanding},
  author={Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={993--1003},
  year={2021}
}

@inproceedings{gu2022xylayoutlm,
  title={Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding},
  author={Gu, Zhangxuan and Meng, Changhua and Wang, Ke and Lan, Jun and Wang, Weiqiang and Gu, Ming and Zhang, Liqing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4583--4592},
  year={2022}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Processing of the 33rd Conference on Neural Information Processing Systems (NeurIPS)},
  pages={1--11},
  year={2019}
}

@inproceedings{yu2021ernie,
  title={Ernie-vil: Knowledge enhanced vision-language representations through scene graphs},
  author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  pages={3208--3216},
  year={2021}
}


@inproceedings{wang2021layoutreader,
  title={LayoutReader: Pre-training of Text and Layout for Reading Order Detection},
  author={Wang, Zilong and Xu, Yiheng and Cui, Lei and Shang, Jingbo and Wei, Furu},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4735--4744},
  year={2021}
}

@inproceedings{zhang2019ernie,
  title={ERNIE: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={1441--1451},
  year={2019}
}

@inproceedings{liu2020kbert,
  title={K-bert: Enabling language representation with knowledge graph},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
  booktitle={Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI)},
  pages={2901--2908},
  year={2020}
}

@inproceedings{he2020bert,
  title={BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models},
  author={He, Bin and Zhou, Di and Xiao, Jinghui and Jiang, Xin and Liu, Qun and Yuan, Nicholas Jing and Xu, Tong},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={2281--2290},
  year={2020}
}

@article{wang2021kepler,
  title={KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  author={Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  volume={9},
  pages={176--194},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{wang2021kadapter,
  title={K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  author={Wang, Ruize and Tang, Duyu and Duan, Nan and Wei, Zhongyu and Huang, Xuan-Jing and Ji, Jianshu and Cao, Guihong and Jiang, Daxin and Zhou, Ming},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1405--1418},
  year={2021}
}

@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

@article{cui2021pre,
  title={Pre-training with whole word masking for chinese bert},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)},
  volume={29},
  pages={3504--3514},
  year={2021}
}

@inproceedings{lee2022formnet,
  title={FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction},
  author={Lee, Chen-Yu and Li, Chun-Liang and Dozat, Timothy and Perot, Vincent and Su, Guolong and Hua, Nan and Ainslie, Joshua and Wang, Renshen and Fujii, Yasuhisa and Pfister, Tomas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={3735--3754},
  year={2022}
}
